import requests
from bs4 import BeautifulSoup
import pandas as pd
import time


def scrape_tg_no_api(channel_name, pages=5):
    base_url = f"https://t.me/s/{channel_name}"
    all_data = []
    current_url = base_url

    # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé˜²æ­¢è¢«æ‹¦æˆª
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    }

    print(f"ğŸš€ æ­£åœ¨è¿æ¥é¢‘é“é¢„è§ˆé¡µ: {channel_name}...")

    for i in range(pages):
        try:
            response = requests.get(current_url, headers=headers, timeout=15)
            if response.status_code != 200:
                print(f"âš ï¸ æ— æ³•è®¿é—®é¡µé¢ï¼ŒçŠ¶æ€ç : {response.status_code}")
                break

            soup = BeautifulSoup(response.text, 'html.parser')

            # è¿™é‡Œçš„é€‰æ‹©å™¨æ˜¯å…³é”®ï¼šTelegram ç½‘é¡µç‰ˆçš„æ¶ˆæ¯å®¹å™¨
            messages = soup.find_all('div', class_='tgme_widget_message')

            if not messages:
                print(f"ğŸ“­ ç¬¬ {i + 1} é¡µæœªå‘ç°æœ‰æ•ˆæ¶ˆæ¯ã€‚å¯èƒ½åˆ°è¾¾äº†å°½å¤´æˆ–è¢«æ‹¦æˆªã€‚")
                # æ‰“å°ä¸€éƒ¨åˆ† HTML çœ‹çœ‹æ˜¯ä¸æ˜¯å‡ºäº†éªŒè¯ç  (æ‰‹åŠ¨æ£€æŸ¥ç”¨)
                # print(response.text[:500])
                break

            for msg in messages:
                # æå–æ–‡æœ¬
                text_div = msg.find('div', class_='tgme_widget_message_text')
                text = text_div.get_text(separator=" ", strip=True) if text_div else None

                # æå–æ—¶é—´ (é‡ç‚¹æ£€æŸ¥è¿™é‡Œ)
                time_tag = msg.find('time', class_='time')
                if time_tag and 'datetime' in time_tag.attrs:
                    dt_str = time_tag['datetime']
                    dt = pd.to_datetime(dt_str).tz_convert('Asia/Shanghai').tz_localize(None)
                else:
                    continue

                if text:
                    all_data.append({'time': dt, 'text': text})

            # å¯»æ‰¾â€œæ˜¾ç¤ºæ›´æ—©æ¶ˆæ¯â€çš„é“¾æ¥
            more_btn = soup.find('a', class_='tme_messages_more', href=True)
            if more_btn and "/s/" in more_btn['href']:
                current_url = "https://t.me" + more_btn['href']
                print(f"ğŸ“… æˆåŠŸæŠ“å–ä¸€æ‰¹æ¶ˆæ¯ï¼Œæ­£åœ¨å›æº¯... (å½“å‰æ•°æ®é‡: {len(all_data)})")
                time.sleep(2)  # ç¨å¾®å¢åŠ å»¶æ—¶ï¼Œæ¨¡æ‹Ÿäººç±»æ“ä½œ
            else:
                break

        except Exception as e:
            print(f"âŒ å¾ªç¯æŠ“å–æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            break

    # --- ä¿®å¤ KeyError çš„æ ¸å¿ƒé€»è¾‘ ---
    if not all_data:
        print("ğŸ›‘ æœ€ç»ˆæœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥é¢‘é“ ID æ˜¯å¦æ­£ç¡®ï¼Œæˆ–åœ¨æµè§ˆå™¨ä¸­å°è¯•è®¿é—®è¯¥é“¾æ¥ã€‚")
        return None

    df = pd.DataFrame(all_data)

    # ç¡®ä¿åˆ—ç¡®å®å­˜åœ¨
    if 'time' in df.columns:
        df = df.drop_duplicates().sort_values('time').reset_index(drop=True)
        df.to_csv('tg_discussions_no_api.csv', index=False)
        print(f"âœ… æŠ“å–å®Œæˆï¼ä¿å­˜äº† {len(df)} æ¡è®¨è®ºã€‚")
        return df
    else:
        print("âŒ æ•°æ®ç»“æ„å¼‚å¸¸ï¼šæ‰¾ä¸åˆ° 'time' åˆ—ã€‚")
        return None


if __name__ == "__main__":
    # å°è¯•æŠ“å– imshards
    scrape_tg_no_api('imshards', pages=5)